{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic (Gaussian likelihood) GP Regression model\n",
    "\n",
    "\n",
    "This notebook has some minor modifications to the [notebook for regression in the original GPflow documentation](https://nbviewer.jupyter.org/github/GPflow/GPflow/blob/develop/doc/source/notebooks/basics/regression.ipynb).\n",
    "\n",
    "The aim of this notebook is to show the different steps for creating and using a standard GP regression model:\n",
    "  - reading and formatting data\n",
    "  - choosing a kernel function\n",
    "  - choosing a mean function\n",
    "  - creating the model\n",
    "  - viewing, getting and setting model parameters\n",
    "  - optimising the model parameters\n",
    "  - making predictions\n",
    "  \n",
    "We focus here on the implementation of the models in GPflow, and refer the reader to [A Practical Guide to Gaussian Processes](https://drafts.distill.pub/gp/) for getting more intuition on these models.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T08:59:50.141046Z",
     "start_time": "2018-06-19T08:59:49.132677Z"
    }
   },
   "outputs": [],
   "source": [
    "import gpflow\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "# The lines below are specific to the notebook format\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 6)\n",
    "plt = matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote by X and Y the input and output values. Note that `X` and `Y` must be two-dimensional numpy arrays, $N \\times 1$ or $N \\times D$, where $D$ is the number of input dimensions/features, with the same number of rows $N$ (one per data-point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('data/regression_1D.csv', delimiter=',')\n",
    "X = data[:, 0].reshape(-1, 1)\n",
    "Y = data[:, 1].reshape(-1, 1)\n",
    "\n",
    "plt.plot(X, Y, 'kx', mew=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the following probabilistic model:\n",
    "$$ Y_i = f(X_i) + \\varepsilon_i , $$\n",
    "where $f \\sim \\mathcal{GP}(\\mu(.), k(., .'))$, and $\\varepsilon \\sim \\mathcal{N}(0, \\tau^2 I)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel \n",
    "--\n",
    "\n",
    "Several kernels (i.e. covariance functions) are implemented in GPflow, and they can easily be combined to create new ones (see [advanced kernel notebook](https://nbviewer.jupyter.org/github/GPflow/GPflow/blob/develop/doc/source/notebooks/advanced/kernels.ipynb)). Implementing new covariance functions is also possible, as illustrated in the [kernel design notebook](https://nbviewer.jupyter.org/github/GPflow/GPflow/blob/develop/doc/source/notebooks/tailor/kernel_design.ipynb). Here, we will use a simple one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = gpflow.kernels.Matern52(input_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `input_dim` parameter is the dimension of the input space. It typically corresponds to the number of columns in `X`  (see the [advanced kernel notebook](https://nbviewer.jupyter.org/github/GPflow/GPflow/blob/develop/doc/source/notebooks/advanced/kernels.ipynb) for kernels defined on subspaces). A summary of the kernel can be obtained, either by `print(k)` (plain text) or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.as_pandas_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Matern 5/2 kernel has two parameters: `lengthscales` encoding the \"wiggliness\" of the GP, and `variance` which tunes the amplitude. They are both set to 1.0 as the default value.  More details on the meaning of the other columns can be found in the [advanced kernel notebook](https://nbviewer.jupyter.org/github/GPflow/GPflow/blob/develop/doc/source/notebooks/advanced/kernels.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean function (optional)\n",
    "--\n",
    "It is common to choose $\\mu = 0$, which is the GPflow default. \n",
    "\n",
    "If there is a clear pattern (such as a mean value of `Y` that is far away from 0, or a linear trend in the data), mean functions can however be beneficial. Some simple ones are provided in the `gpflow.mean_functions` module. Here's how to define a linear mean function: ` meanf = gpflow.mean_functions.Linear()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model construction\n",
    "--\n",
    "\n",
    "A GPflow model is created by instantiating one of the GPflow model classes, in this case GPR. We'll make a kernel `k` and instantiate a GPR object using the generated data and the kernel. We'll set the variance of the likelihood to a sensible initial guess, too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T08:59:50.402776Z",
     "start_time": "2018-06-19T08:59:50.292900Z"
    }
   },
   "outputs": [],
   "source": [
    "m = gpflow.models.GPR(X, Y, kern=k, mean_function=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the model can be obtained, either by `print(m)` (plain text) or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.as_pandas_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two lines correspond to the kernel parameters, and the third one gives the likelihood parameter (the noise variance $\\tau^2$ in our model).\n",
    "\n",
    "Those values can be accessed and set manually to sensible initial guesses, for instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.likelihood.variance = 0.01\n",
    "m.kern.lengthscales = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation of the model parameters\n",
    "--\n",
    "\n",
    "In order to obtain meaningful predictions, we need to tune the model parameters (i.e. parameters of the kernel, likelihood and mean function if applicable) to the data at hand. \n",
    "\n",
    "There are several optimisers available in GPflow. Here we use the ScipyOptimizer which implements by default the L-BFGS-B algorithm (others can be selected using the `method=` keyword argument to `ScipyOptimizer`, for options see [the Scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = gpflow.train.ScipyOptimizer()\n",
    "opt.minimize(m)\n",
    "m.as_pandas_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the value column has changed.\n",
    "\n",
    "The local optimum found by Maximum Likelihood may not be the one you want, e.g. it may be overfitting or be oversmooth. This depends on the initial values of hyperparameters and is specific to each data set. As an alternative to Maximum Likelihood, MCMC is also available as shown in the [MCMC notebook](https://nbviewer.jupyter.org/github/GPflow/GPflow/blob/develop/doc/source/notebooks/advanced/mcmc.ipynb).\n",
    "\n",
    "### Prediction\n",
    "\n",
    "We can now use the model to make some predictions at new locations `Xnew`. One may be interested in predicting two different quantities: the latent function values `f(Xnew)` (the denoised signal), or the values of new observations `y(Xnew)` (signal + noise). Since we are dealing with Gaussian probabilistic models, the predictions typically output a mean and variance. Alternatively, one can obtain samples of `f(Xnew)` or log-density of new data points `(Xnew, Ynew)`.\n",
    "\n",
    "GPflow models have several prediction methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `m.predict_f` returns the mean and variance of $f$ at the points `Xnew`. \n",
    "\n",
    " - `m.predict_f_full_cov` additionally returns the full covariance matrix of $f$ at the points `Xnew`.\n",
    "\n",
    " - `m.predict_f_samples` returns samples of the latent function.\n",
    "\n",
    " - `m.predict_y` returns the mean and variance of a new data point (i.e. includes the noise variance).\n",
    "\n",
    " - `m.predict_density` returns the log-density of the observations `Ynew` at `Xnew`.\n",
    " \n",
    "We use `predict_f` and `predict_f_samples` to plot 95% confidence intervals and samples from the posterior distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T08:59:50.640000Z",
     "start_time": "2018-06-19T08:59:50.404378Z"
    }
   },
   "outputs": [],
   "source": [
    "## generate test points for prediction\n",
    "xx = np.linspace(-0.1, 1.1, 100).reshape(100, 1)  # test points must be of shape (N, D)\n",
    "\n",
    "## predict mean and variance of latent GP at test points\n",
    "mean, var = m.predict_f(xx)\n",
    "\n",
    "## generate 10 samples from posterior\n",
    "samples = m.predict_f_samples(xx, 10)  # shape (10, 100, 1)\n",
    "\n",
    "## plot \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(X, Y, 'kx', mew=2)\n",
    "plt.plot(xx, mean, 'C0', lw=2)\n",
    "plt.fill_between(xx[:,0],\n",
    "                 mean[:,0] - 1.96 * np.sqrt(var[:,0]),\n",
    "                 mean[:,0] + 1.96 * np.sqrt(var[:,0]),\n",
    "                 color='C0', alpha=0.2)\n",
    "\n",
    "plt.plot(xx, samples[:, :, 0].T, 'C0', linewidth=.5)\n",
    "plt.xlim(-0.1, 1.1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GP regression in higher dimension\n",
    "--\n",
    "\n",
    "Very little changes when the input space has more than one dimension. `X` is a numpy array with one column per dimension. The kernel may be set with `input_dim` equal to the number of columns of `X`, and setting the parameter `ARD=True` allows tuning a different lengthscale per dimension, which is generally recommended."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
